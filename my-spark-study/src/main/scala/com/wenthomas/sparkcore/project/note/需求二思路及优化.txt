/*
记算热门session口径: 看每个session的点击记录
1.过滤出来只包含top10品类的那些点击记录
RDD[UserVisitAction]
2. 每个品类top10session的计算
=> RDD[((cid, sid), 1))] reduceByKey
=> RDD[((cid, sid), count))] map
=> RDD[(cid, (sid, count))]  groupByKey
RDD[(cid, Iterator[(sessionId, count), (sessionId, count),...])]  map内部, 对iterator排序, 取前10

-----

使用scala的排序, 会导致内存溢出
问题解决方案:
    方案2:
        1. 用spark排序,来解决问题.
        2. spark的排序是整体排序. 不能直接使用spark排序
        3. 10个品类id, 我就使用spark的排序功能排10次.

        优点:
            一定能完成, 不会oom
        缺点:
            起 10 job, 排序10次

     方案3:
        内存溢出, iterable => 转换list

        最终的目的top10
        搞一个集合, 这集合中永远只保存10个元素, 用于最大的10 个元素

        先聚合, 聚合后分组, 分组内做了排序(用了自动排序的功能集合TreeSet)

        优点:
            一定可以完成, 也不会oom, job也是只有一个job

        坏处:
            做了两次shuffle, 效率受影响

      方案4:
        对方案3做优化, 减少一次shuffle
        减少shuffle只能是去掉groupByKey

        还得需要得到每个cid的所有session的集合?! 怎么得?
        rdd是分区的, mapPartitions(it => {})
        能不能让一个分区只有一个cid的所有数据

        每个分区只有一种cid, 如何做到每个分区只有一个cid?
        用自定义分区器!
        10cid, 应该有10个分区








 */